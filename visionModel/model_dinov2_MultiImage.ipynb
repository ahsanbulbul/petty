{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d90927e",
   "metadata": {},
   "source": [
    "# ðŸ¾ Multi-Image Pet Re-Identification System with DINOv2\n",
    "## Enhanced Accuracy with Multiple Image Inputs\n",
    "\n",
    "This notebook implements an **advanced multi-image re-identification pipeline** using **DINOv2** for superior matching accuracy:\n",
    "\n",
    "### ðŸŽ¯ Key Features:\n",
    "1. **Multiple Lost Images**: Images a, b, c from \"lost\" post\n",
    "2. **Multiple Found Images**: Images p, q from \"found\" post\n",
    "3. **Ensemble Matching**: Uses all images to compute robust similarity\n",
    "4. **DINOv2 Embeddings**: State-of-the-art visual similarity (better than MegaDescriptor!)\n",
    "5. **Confidence Scoring**: Shows which image pairs contribute most to match decision\n",
    "6. **Visual Inspection**: Compare all image combinations with detailed visualizations\n",
    "\n",
    "### ðŸ“Š Pipeline:\n",
    "**YOLOv8-Seg Detection** â†’ **Extract DINOv2 Embeddings** â†’ **Compute Similarity Matrix** â†’ **Aggregate Decision** â†’ **Visual Report**\n",
    "\n",
    "### ðŸ’¡ Why Multi-Image + DINOv2 is Better:\n",
    "- **DINOv2**: Superior visual similarity features (0.65-0.85 for matches vs 0.45-0.70 for MegaDescriptor)\n",
    "- Different angles provide complementary information\n",
    "- Reduces impact of poor lighting/pose in single image\n",
    "- More robust to occlusions and partial views\n",
    "- Handles variation in pet appearance (sitting vs standing)\n",
    "- Higher confidence in final match decision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f119883",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e52af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ultralytics transformers torch torchvision pillow matplotlib opencv-python numpy scikit-learn seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eb2716",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b065966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from ultralytics import YOLO\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "import cv2\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a131b3bd",
   "metadata": {},
   "source": [
    "## 3. Load Models (YOLOv8-Seg + DINOv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8773ea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Setup cache directory\n",
    "model_cache_dir = Path('models_cache')\n",
    "model_cache_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Load YOLOv8-Seg\n",
    "print('Loading YOLOv8-Seg...')\n",
    "yolo_model_path = model_cache_dir / 'yolov8x-seg.pt'\n",
    "if yolo_model_path.exists():\n",
    "    yolo_model = YOLO(str(yolo_model_path))\n",
    "    print('âœ“ Using cached YOLOv8-Seg')\n",
    "else:\n",
    "    yolo_model = YOLO('yolov8x-seg.pt')\n",
    "    yolo_model.save(str(yolo_model_path))\n",
    "    print('âœ“ YOLOv8-Seg downloaded and cached')\n",
    "\n",
    "PET_CLASSES = {15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', \n",
    "               20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe'}\n",
    "\n",
    "# Load DINOv2 (Better than MegaDescriptor for visual similarity!)\n",
    "print('\\nLoading DINOv2-Large...')\n",
    "dino_model_name = 'facebook/dinov2-large'\n",
    "dino_cache_dir = model_cache_dir / 'dinov2'\n",
    "dino_cache_dir.mkdir(exist_ok=True)\n",
    "\n",
    "os.environ['TRANSFORMERS_CACHE'] = str(dino_cache_dir.absolute())\n",
    "os.environ['HF_HOME'] = str(dino_cache_dir.absolute())\n",
    "\n",
    "# Check if model is already cached\n",
    "model_files = list(dino_cache_dir.rglob('*.bin')) or list(dino_cache_dir.rglob('*.safetensors'))\n",
    "if model_files:\n",
    "    print(f'âœ“ Using cached DINOv2 from {dino_cache_dir}')\n",
    "else:\n",
    "    print('â¬‡ï¸  Downloading DINOv2-Large (~1.2GB, first time only)...')\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    dino_model_name,\n",
    "    cache_dir=str(dino_cache_dir)\n",
    ")\n",
    "reid_model = AutoModel.from_pretrained(\n",
    "    dino_model_name,\n",
    "    cache_dir=str(dino_cache_dir)\n",
    ")\n",
    "reid_model.eval()\n",
    "reid_model = reid_model.to(device)\n",
    "\n",
    "print('\\nâœ… All models loaded successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590adf6d",
   "metadata": {},
   "source": [
    "## 4. Core Functions (Detection & Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41c4a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_bbox(mask, width, height):\n",
    "    \"\"\"Convert segmentation mask to bounding box\"\"\"\n",
    "    mask_resized = cv2.resize(mask, (width, height), interpolation=cv2.INTER_LINEAR)\n",
    "    binary = mask_resized > 0.45\n",
    "    ys, xs = np.where(binary)\n",
    "    if len(xs) == 0 or len(ys) == 0:\n",
    "        return None, None\n",
    "    x1, x2 = xs.min(), xs.max()\n",
    "    y1, y2 = ys.min(), ys.max()\n",
    "    return (x1, y1, x2, y2), binary\n",
    "\n",
    "def detect_pets_with_segmentation(image_path, confidence_threshold=0.4):\n",
    "    \"\"\"Detect pets using YOLOv8-Seg\"\"\"\n",
    "    pil_img = Image.open(image_path).convert('RGB')\n",
    "    img_array = np.array(pil_img)\n",
    "    \n",
    "    results = yolo_model(img_array, verbose=False)[0]\n",
    "    \n",
    "    if results.masks is None:\n",
    "        return pil_img, img_array, []\n",
    "    \n",
    "    detections = []\n",
    "    for i, (box, mask) in enumerate(zip(results.boxes, results.masks.data)):\n",
    "        cls_id = int(box.cls[0])\n",
    "        confidence = float(box.conf[0])\n",
    "        \n",
    "        if cls_id in PET_CLASSES and confidence >= confidence_threshold:\n",
    "            mask_np = mask.cpu().numpy()\n",
    "            bbox, binary_mask = mask_to_bbox(mask_np, img_array.shape[1], img_array.shape[0])\n",
    "            \n",
    "            if bbox is not None:\n",
    "                detections.append({\n",
    "                    'bbox': bbox,\n",
    "                    'mask': binary_mask,\n",
    "                    'confidence': confidence,\n",
    "                    'class_id': cls_id,\n",
    "                    'class_name': PET_CLASSES[cls_id]\n",
    "                })\n",
    "    \n",
    "    return pil_img, img_array, detections\n",
    "\n",
    "def preprocess_crop_advanced(img_array, bbox, mask=None, padding_ratio=0.15, enhance_contrast=True):\n",
    "    \"\"\"Advanced preprocessing with background removal and enhancement\"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    h, w = img_array.shape[:2]\n",
    "    \n",
    "    box_w, box_h = x2 - x1, y2 - y1\n",
    "    pad_x, pad_y = int(box_w * padding_ratio), int(box_h * padding_ratio)\n",
    "    \n",
    "    x1_pad = max(0, x1 - pad_x)\n",
    "    y1_pad = max(0, y1 - pad_y)\n",
    "    x2_pad = min(w, x2 + pad_x)\n",
    "    y2_pad = min(h, y2 + pad_y)\n",
    "    \n",
    "    cropped = img_array[y1_pad:y2_pad, x1_pad:x2_pad].copy()\n",
    "    \n",
    "    if mask is not None:\n",
    "        mask_crop = mask[y1_pad:y2_pad, x1_pad:x2_pad]\n",
    "        mask_3ch = np.stack([mask_crop] * 3, axis=-1)\n",
    "        gray_bg = np.full_like(cropped, 127)\n",
    "        cropped = np.where(mask_3ch, cropped, gray_bg)\n",
    "    \n",
    "    if enhance_contrast:\n",
    "        cropped_lab = cv2.cvtColor(cropped, cv2.COLOR_RGB2LAB)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        cropped_lab[:, :, 0] = clahe.apply(cropped_lab[:, :, 0])\n",
    "        cropped = cv2.cvtColor(cropped_lab, cv2.COLOR_LAB2RGB)\n",
    "    \n",
    "    return Image.fromarray(cropped)\n",
    "\n",
    "def extract_embedding_with_tta(cropped_image, use_augmentation=True):\n",
    "    \"\"\"\n",
    "    Extract embedding with test-time augmentation using DINOv2.\n",
    "    DINOv2 provides superior visual similarity features compared to MegaDescriptor.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    # Original image\n",
    "    inputs = image_processor(images=cropped_image, return_tensors='pt')\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = reid_model(**inputs)\n",
    "        # Use [CLS] token embedding from DINOv2\n",
    "        embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    # Horizontal flip augmentation (helps with left/right facing pets)\n",
    "    if use_augmentation:\n",
    "        flipped = cropped_image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        inputs_flip = image_processor(images=flipped, return_tensors='pt')\n",
    "        inputs_flip = {k: v.to(device) for k, v in inputs_flip.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs_flip = reid_model(**inputs_flip)\n",
    "            embedding_flip = outputs_flip.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(embedding_flip)\n",
    "    \n",
    "    # Average embeddings from augmentations\n",
    "    final_embedding = np.mean(embeddings, axis=0).astype(np.float32)\n",
    "    \n",
    "    # L2 normalize\n",
    "    final_embedding = final_embedding / (np.linalg.norm(final_embedding) + 1e-8)\n",
    "    \n",
    "    return final_embedding.flatten()\n",
    "\n",
    "print('âœ… Core functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c997b4c2",
   "metadata": {},
   "source": [
    "## 5. Multi-Image Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e428012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_batch(image_paths, label=\"Images\"):\n",
    "    \"\"\"\n",
    "    Process multiple images and extract embeddings from all detected pets\n",
    "    \n",
    "    Args:\n",
    "        image_paths: List of image file paths\n",
    "        label: Label for this batch (e.g., \"Lost\", \"Found\")\n",
    "    \n",
    "    Returns:\n",
    "        List of dicts with image data, detections, crops, and embeddings\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing {label} Images: {len(image_paths)} images\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx, img_path in enumerate(image_paths):\n",
    "        print(f\"\\n[{idx+1}/{len(image_paths)}] Processing {Path(img_path).name}...\")\n",
    "        \n",
    "        pil_img, img_array, detections = detect_pets_with_segmentation(img_path)\n",
    "        \n",
    "        if len(detections) == 0:\n",
    "            print(f\"  âš ï¸  No pets detected in {Path(img_path).name}\")\n",
    "            continue\n",
    "        \n",
    "        # Process the first (most confident) detection\n",
    "        det = detections[0]\n",
    "        print(f\"  âœ“ Detected: {det['class_name']} (confidence: {det['confidence']:.2f})\")\n",
    "        \n",
    "        # Preprocess and extract embedding\n",
    "        crop = preprocess_crop_advanced(img_array, det['bbox'], det.get('mask'))\n",
    "        embedding = extract_embedding_with_tta(crop, use_augmentation=True)\n",
    "        \n",
    "        results.append({\n",
    "            'image_path': img_path,\n",
    "            'image_name': Path(img_path).name,\n",
    "            'pil_img': pil_img,\n",
    "            'img_array': img_array,\n",
    "            'detection': det,\n",
    "            'crop': crop,\n",
    "            'embedding': embedding\n",
    "        })\n",
    "        \n",
    "        print(f\"  âœ“ Embedding extracted (dim: {embedding.shape[0]})\")\n",
    "    \n",
    "    print(f\"\\nâœ… Processed {len(results)}/{len(image_paths)} images successfully\")\n",
    "    return results\n",
    "\n",
    "print('âœ… Multi-image processing functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd4fb0",
   "metadata": {},
   "source": [
    "## 6. Similarity Matrix Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae8dec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_matrix(lost_results, found_results):\n",
    "    \"\"\"\n",
    "    Compute pairwise similarity matrix between all lost and found images\n",
    "    \n",
    "    Args:\n",
    "        lost_results: List of processed lost images\n",
    "        found_results: List of processed found images\n",
    "    \n",
    "    Returns:\n",
    "        similarity_matrix: numpy array of shape (n_lost, n_found)\n",
    "        lost_names: List of lost image names\n",
    "        found_names: List of found image names\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Computing Similarity Matrix\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    n_lost = len(lost_results)\n",
    "    n_found = len(found_results)\n",
    "    \n",
    "    similarity_matrix = np.zeros((n_lost, n_found))\n",
    "    lost_names = [r['image_name'] for r in lost_results]\n",
    "    found_names = [r['image_name'] for r in found_results]\n",
    "    \n",
    "    print(f\"\\nComparing {n_lost} lost images with {n_found} found images...\\n\")\n",
    "    \n",
    "    for i, lost in enumerate(lost_results):\n",
    "        for j, found in enumerate(found_results):\n",
    "            emb1 = lost['embedding']\n",
    "            emb2 = found['embedding']\n",
    "            \n",
    "            # Compute cosine similarity\n",
    "            sim = cosine_similarity(\n",
    "                emb1.reshape(1, -1),\n",
    "                emb2.reshape(1, -1)\n",
    "            )[0][0]\n",
    "            \n",
    "            similarity_matrix[i, j] = sim\n",
    "            print(f\"  {lost_names[i]:15s} â†” {found_names[j]:15s}: {sim:.4f}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Similarity matrix computed\")\n",
    "    return similarity_matrix, lost_names, found_names\n",
    "\n",
    "print('âœ… Similarity matrix computation defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54102154",
   "metadata": {},
   "source": [
    "## 7. Ensemble Matching & Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8921566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ensemble_similarity(similarity_matrix, aggregation='max'):\n",
    "    \"\"\"\n",
    "    Compute ensemble similarity score from multiple images\n",
    "    \n",
    "    Args:\n",
    "        similarity_matrix: (n_lost, n_found) similarity scores\n",
    "        aggregation: Method to aggregate scores\n",
    "            - 'max': Take maximum similarity (best matching pair)\n",
    "            - 'mean': Take average similarity (overall agreement)\n",
    "            - 'median': Take median similarity (robust to outliers)\n",
    "            - 'top2_mean': Average of top 2 scores (balanced approach)\n",
    "    \n",
    "    Returns:\n",
    "        ensemble_score: Single similarity score\n",
    "        details: Dictionary with breakdown\n",
    "    \"\"\"\n",
    "    flat_scores = similarity_matrix.flatten()\n",
    "    \n",
    "    if aggregation == 'max':\n",
    "        score = np.max(flat_scores)\n",
    "    elif aggregation == 'mean':\n",
    "        score = np.mean(flat_scores)\n",
    "    elif aggregation == 'median':\n",
    "        score = np.median(flat_scores)\n",
    "    elif aggregation == 'top2_mean':\n",
    "        top_scores = np.sort(flat_scores)[-2:] if len(flat_scores) >= 2 else flat_scores\n",
    "        score = np.mean(top_scores)\n",
    "    else:\n",
    "        score = np.max(flat_scores)  # Default to max\n",
    "    \n",
    "    details = {\n",
    "        'max': np.max(flat_scores),\n",
    "        'mean': np.mean(flat_scores),\n",
    "        'median': np.median(flat_scores),\n",
    "        'min': np.min(flat_scores),\n",
    "        'std': np.std(flat_scores),\n",
    "        'all_scores': flat_scores.tolist()\n",
    "    }\n",
    "    \n",
    "    return score, details\n",
    "\n",
    "def make_match_decision(similarity_matrix, lost_names, found_names, \n",
    "                       threshold=0.55, aggregation='max'):\n",
    "    \"\"\"\n",
    "    Make final match decision based on ensemble similarity\n",
    "    \n",
    "    Returns:\n",
    "        match: Boolean indicating if pets match\n",
    "        score: Ensemble similarity score\n",
    "        details: Detailed information\n",
    "    \"\"\"\n",
    "    ensemble_score, details = compute_ensemble_similarity(similarity_matrix, aggregation)\n",
    "    \n",
    "    match = ensemble_score >= threshold\n",
    "    \n",
    "    # Find best matching pair\n",
    "    best_idx = np.unravel_index(np.argmax(similarity_matrix), similarity_matrix.shape)\n",
    "    best_lost = lost_names[best_idx[0]]\n",
    "    best_found = found_names[best_idx[1]]\n",
    "    best_score = similarity_matrix[best_idx]\n",
    "    \n",
    "    return match, ensemble_score, {\n",
    "        'aggregation': aggregation,\n",
    "        'ensemble_score': ensemble_score,\n",
    "        'threshold': threshold,\n",
    "        'match': match,\n",
    "        'best_pair': (best_lost, best_found),\n",
    "        'best_score': best_score,\n",
    "        'statistics': details\n",
    "    }\n",
    "\n",
    "print('âœ… Ensemble matching functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aba12a",
   "metadata": {},
   "source": [
    "## 8. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263203ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_all_detections(results, title=\"Detections\"):\n",
    "    \"\"\"\n",
    "    Visualize all detected pets in a grid\n",
    "    \"\"\"\n",
    "    n = len(results)\n",
    "    if n == 0:\n",
    "        print(\"No results to visualize\")\n",
    "        return\n",
    "    \n",
    "    cols = min(3, n)\n",
    "    rows = (n + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(6*cols, 5*rows))\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten() if n > cols else axes\n",
    "    \n",
    "    for idx, result in enumerate(results):\n",
    "        ax = axes[idx]\n",
    "        ax.imshow(result['img_array'])\n",
    "        \n",
    "        det = result['detection']\n",
    "        x1, y1, x2, y2 = det['bbox']\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1,\n",
    "                                linewidth=3, edgecolor='lime', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        label = f\"{result['image_name']}\\n{det['class_name']} ({det['confidence']:.2f})\"\n",
    "        ax.set_title(label, fontsize=12, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(len(results), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_similarity_matrix(similarity_matrix, lost_names, found_names):\n",
    "    \"\"\"\n",
    "    Visualize similarity matrix as heatmap\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    sns.heatmap(similarity_matrix, \n",
    "                xticklabels=found_names,\n",
    "                yticklabels=lost_names,\n",
    "                annot=True, fmt='.3f',\n",
    "                cmap='RdYlGn', vmin=0, vmax=1,\n",
    "                cbar_kws={'label': 'Similarity'},\n",
    "                linewidths=1, linecolor='white')\n",
    "    \n",
    "    plt.title('Similarity Matrix: Lost vs Found Images', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Found Images', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Lost Images', fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_best_matches(lost_results, found_results, similarity_matrix, \n",
    "                          lost_names, found_names, top_k=3):\n",
    "    \"\"\"\n",
    "    Visualize top-k best matching pairs\n",
    "    \"\"\"\n",
    "    # Find top-k matches\n",
    "    flat_indices = np.argsort(similarity_matrix.flatten())[::-1][:top_k]\n",
    "    top_indices = [np.unravel_index(idx, similarity_matrix.shape) for idx in flat_indices]\n",
    "    \n",
    "    fig, axes = plt.subplots(top_k, 3, figsize=(18, 6*top_k))\n",
    "    if top_k == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for rank, (i, j) in enumerate(top_indices):\n",
    "        sim = similarity_matrix[i, j]\n",
    "        lost = lost_results[i]\n",
    "        found = found_results[j]\n",
    "        \n",
    "        # Lost crop\n",
    "        axes[rank][0].imshow(lost['crop'])\n",
    "        axes[rank][0].set_title(f\"Lost: {lost['image_name']}\", fontsize=12, fontweight='bold')\n",
    "        axes[rank][0].axis('off')\n",
    "        \n",
    "        # Similarity score (DINOv2 has higher thresholds than MegaDescriptor)\n",
    "        color = 'green' if sim >= 0.65 else 'orange' if sim >= 0.50 else 'red'\n",
    "        status = 'âœ“ MATCH' if sim >= 0.65 else '? UNCERTAIN' if sim >= 0.50 else 'âœ— NO MATCH'\n",
    "        \n",
    "        axes[rank][1].text(0.5, 0.6, f\"Rank #{rank+1}\", \n",
    "                          ha='center', va='center', fontsize=20, fontweight='bold')\n",
    "        axes[rank][1].text(0.5, 0.45, status,\n",
    "                          ha='center', va='center', fontsize=22, fontweight='bold', color=color)\n",
    "        axes[rank][1].text(0.5, 0.3, f\"Similarity: {sim:.4f}\",\n",
    "                          ha='center', va='center', fontsize=18)\n",
    "        axes[rank][1].set_xlim(0, 1)\n",
    "        axes[rank][1].set_ylim(0, 1)\n",
    "        axes[rank][1].axis('off')\n",
    "        \n",
    "        # Found crop\n",
    "        axes[rank][2].imshow(found['crop'])\n",
    "        axes[rank][2].set_title(f\"Found: {found['image_name']}\", fontsize=12, fontweight='bold')\n",
    "        axes[rank][2].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Top {top_k} Best Matching Pairs', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_ensemble_result(decision_details, similarity_matrix, lost_names, found_names):\n",
    "    \"\"\"\n",
    "    Visualize final ensemble decision with statistics\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    # Main result\n",
    "    ax1 = plt.subplot(2, 2, (1, 2))\n",
    "    \n",
    "    match = decision_details['match']\n",
    "    score = decision_details['ensemble_score']\n",
    "    threshold = decision_details['threshold']\n",
    "    \n",
    "    color = 'green' if match else 'red'\n",
    "    status = 'âœ… MATCH' if match else 'âŒ NO MATCH'\n",
    "    \n",
    "    ax1.text(0.5, 0.7, status, ha='center', va='center',\n",
    "            fontsize=48, fontweight='bold', color=color)\n",
    "    ax1.text(0.5, 0.5, f'Ensemble Score: {score:.4f}', ha='center', va='center',\n",
    "            fontsize=32, fontweight='bold')\n",
    "    ax1.text(0.5, 0.35, f'Threshold: {threshold}', ha='center', va='center',\n",
    "            fontsize=20, color='gray')\n",
    "    ax1.text(0.5, 0.25, f\"Aggregation: {decision_details['aggregation']}\", ha='center', va='center',\n",
    "            fontsize=16, color='gray', style='italic')\n",
    "    \n",
    "    best_lost, best_found = decision_details['best_pair']\n",
    "    best_score = decision_details['best_score']\n",
    "    ax1.text(0.5, 0.1, f\"Best pair: {best_lost} â†” {best_found} ({best_score:.4f})\",\n",
    "            ha='center', va='center', fontsize=14, color='blue')\n",
    "    \n",
    "    ax1.set_xlim(0, 1)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Statistics\n",
    "    ax2 = plt.subplot(2, 2, 3)\n",
    "    stats = decision_details['statistics']\n",
    "    \n",
    "    stats_text = f\"\"\"\n",
    "ðŸ“Š Similarity Statistics:\n",
    "\n",
    "Maximum:  {stats['max']:.4f}\n",
    "Mean:     {stats['mean']:.4f}\n",
    "Median:   {stats['median']:.4f}\n",
    "Minimum:  {stats['min']:.4f}\n",
    "Std Dev:  {stats['std']:.4f}\n",
    "\n",
    "Total comparisons: {len(stats['all_scores'])}\n",
    "    \"\"\"\n",
    "    \n",
    "    ax2.text(0.1, 0.5, stats_text, ha='left', va='center',\n",
    "            fontsize=14, family='monospace',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "    ax2.set_xlim(0, 1)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    # Score distribution\n",
    "    ax3 = plt.subplot(2, 2, 4)\n",
    "    all_scores = stats['all_scores']\n",
    "    ax3.hist(all_scores, bins=20, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    ax3.axvline(threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold ({threshold})')\n",
    "    ax3.axvline(score, color='green', linestyle='-', linewidth=2, label=f'Ensemble ({score:.3f})')\n",
    "    ax3.set_xlabel('Similarity Score', fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "    ax3.set_title('Score Distribution', fontsize=14, fontweight='bold')\n",
    "    ax3.legend()\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Multi-Image Ensemble Re-ID Result', fontsize=18, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print('âœ… Visualization functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff616922",
   "metadata": {},
   "source": [
    "## 9. Complete Multi-Image Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a04953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_image_reid_pipeline(lost_images, found_images, \n",
    "                              similarity_threshold=0.65,\n",
    "                              aggregation='max',\n",
    "                              show_all_visualizations=True):\n",
    "    \"\"\"\n",
    "    Complete multi-image pet re-identification pipeline using DINOv2\n",
    "    \n",
    "    Args:\n",
    "        lost_images: List of paths to \"lost\" pet images (e.g., ['a.jpg', 'b.jpg', 'c.jpg'])\n",
    "        found_images: List of paths to \"found\" pet images (e.g., ['p.jpg', 'q.jpg'])\n",
    "        similarity_threshold: Threshold for match decision (DINOv2: 0.60-0.75, default: 0.65)\n",
    "        aggregation: How to combine scores ('max', 'mean', 'median', 'top2_mean')\n",
    "        show_all_visualizations: Whether to show all visualization steps\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with complete results\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ¾ MULTI-IMAGE PET RE-IDENTIFICATION PIPELINE (DINOv2)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"  Model: DINOv2-Large (facebook/dinov2-large)\")\n",
    "    print(f\"  Lost images:  {len(lost_images)} images\")\n",
    "    print(f\"  Found images: {len(found_images)} images\")\n",
    "    print(f\"  Total comparisons: {len(lost_images) * len(found_images)}\")\n",
    "    print(f\"  Threshold: {similarity_threshold}\")\n",
    "    print(f\"  Aggregation: {aggregation}\")\n",
    "    \n",
    "    # Step 1: Process lost images\n",
    "    lost_results = process_image_batch(lost_images, \"Lost\")\n",
    "    if len(lost_results) == 0:\n",
    "        print(\"\\nâš ï¸  No pets detected in lost images!\")\n",
    "        return None\n",
    "    \n",
    "    # Step 2: Process found images\n",
    "    found_results = process_image_batch(found_images, \"Found\")\n",
    "    if len(found_results) == 0:\n",
    "        print(\"\\nâš ï¸  No pets detected in found images!\")\n",
    "        return None\n",
    "    \n",
    "    if show_all_visualizations:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"VISUALIZING DETECTIONS\")\n",
    "        print(\"=\"*80)\n",
    "        visualize_all_detections(lost_results, \"Lost Pet Images - Detections\")\n",
    "        visualize_all_detections(found_results, \"Found Pet Images - Detections\")\n",
    "    \n",
    "    # Step 3: Compute similarity matrix\n",
    "    similarity_matrix, lost_names, found_names = compute_similarity_matrix(\n",
    "        lost_results, found_results\n",
    "    )\n",
    "    \n",
    "    if show_all_visualizations:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"VISUALIZING SIMILARITY MATRIX\")\n",
    "        print(\"=\"*80)\n",
    "        visualize_similarity_matrix(similarity_matrix, lost_names, found_names)\n",
    "    \n",
    "    # Step 4: Make ensemble decision\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MAKING ENSEMBLE DECISION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    match, ensemble_score, decision_details = make_match_decision(\n",
    "        similarity_matrix, lost_names, found_names,\n",
    "        threshold=similarity_threshold,\n",
    "        aggregation=aggregation\n",
    "    )\n",
    "    \n",
    "    # Step 5: Visualize results\n",
    "    if show_all_visualizations:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"VISUALIZING BEST MATCHES\")\n",
    "        print(\"=\"*80)\n",
    "        top_k = min(3, len(lost_results) * len(found_results))\n",
    "        visualize_best_matches(lost_results, found_results, similarity_matrix,\n",
    "                             lost_names, found_names, top_k=top_k)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL ENSEMBLE RESULT\")\n",
    "    print(\"=\"*80)\n",
    "    visualize_ensemble_result(decision_details, similarity_matrix, lost_names, found_names)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“‹ SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nDecision: {'âœ… PETS MATCH' if match else 'âŒ PETS DO NOT MATCH'}\")\n",
    "    print(f\"Ensemble Score: {ensemble_score:.4f}\")\n",
    "    print(f\"Threshold: {similarity_threshold}\")\n",
    "    print(f\"\\nBest matching pair:\")\n",
    "    print(f\"  {decision_details['best_pair'][0]} â†” {decision_details['best_pair'][1]}\")\n",
    "    print(f\"  Similarity: {decision_details['best_score']:.4f}\")\n",
    "    \n",
    "    stats = decision_details['statistics']\n",
    "    print(f\"\\nAll pairwise similarities:\")\n",
    "    print(f\"  Max:    {stats['max']:.4f}\")\n",
    "    print(f\"  Mean:   {stats['mean']:.4f}\")\n",
    "    print(f\"  Median: {stats['median']:.4f}\")\n",
    "    print(f\"  Min:    {stats['min']:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    return {\n",
    "        'match': match,\n",
    "        'ensemble_score': ensemble_score,\n",
    "        'decision_details': decision_details,\n",
    "        'similarity_matrix': similarity_matrix,\n",
    "        'lost_results': lost_results,\n",
    "        'found_results': found_results,\n",
    "        'lost_names': lost_names,\n",
    "        'found_names': found_names\n",
    "    }\n",
    "\n",
    "print('âœ… Complete multi-image pipeline defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a1f52c",
   "metadata": {},
   "source": [
    "## 10. Visual Inspection of Images (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31b03c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick visual inspection of available images\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# List available images\n",
    "image_dir = Path('images')\n",
    "if image_dir.exists():\n",
    "    all_images = sorted([str(f) for f in image_dir.glob('*.jpg')])\n",
    "    print(\"Available images:\")\n",
    "    for img in all_images:\n",
    "        print(f\"  - {Path(img).name}\")\n",
    "else:\n",
    "    all_images = []\n",
    "    print(\"Images directory not found. Using current directory.\")\n",
    "    all_images = sorted([f for f in os.listdir('.') if f.endswith('.jpg')])\n",
    "    for img in all_images:\n",
    "        print(f\"  - {img}\")\n",
    "\n",
    "# Display images in a grid for visual inspection\n",
    "if all_images:\n",
    "    print(f\"\\nðŸ“¸ Displaying {len(all_images)} images for visual inspection...\")\n",
    "    \n",
    "    cols = 4\n",
    "    rows = (len(all_images) + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 3*rows))\n",
    "    axes = axes.flatten() if len(all_images) > 1 else [axes]\n",
    "    \n",
    "    for idx, img_path in enumerate(all_images):\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "            axes[idx].imshow(img)\n",
    "            axes[idx].set_title(Path(img_path).name, fontsize=10)\n",
    "            axes[idx].axis('off')\n",
    "        except Exception as e:\n",
    "            axes[idx].text(0.5, 0.5, f\"Error: {Path(img_path).name}\", \n",
    "                         ha='center', va='center')\n",
    "            axes[idx].axis('off')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(len(all_images), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle('Available Images for Re-ID', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No images found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680f5e73",
   "metadata": {},
   "source": [
    "## 11. Run Multi-Image Pipeline\n",
    "\n",
    "### ðŸ“Œ Define your image groups:\n",
    "- **Lost images**: Multiple photos from the \"lost\" post (e.g., a.jpg, b.jpg, c.jpg)\n",
    "- **Found images**: Multiple photos from the \"found\" post (e.g., p.jpg, q.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591e96d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your image sets\n",
    "# Adjust these paths according to your images\n",
    "\n",
    "# Option 1: If images are in 'images/' subdirectory\n",
    "lost_images = [\n",
    "    # 'images/a.jpg',\n",
    "    # 'images/b.jpg',\n",
    "    # 'images/c.jpg'\n",
    "    'images/a_3653039964999248.png'\n",
    "]\n",
    "\n",
    "found_images = [\n",
    "    # 'images/p.jpg',\n",
    "    # 'images/q.jpg'\n",
    "    'images/b_3653039964999248.png'\n",
    "]\n",
    "\n",
    "# Option 2: If images are in current directory\n",
    "# lost_images = ['a.jpg', 'b.jpg', 'c.jpg']\n",
    "# found_images = ['p.jpg', 'q.jpg']\n",
    "\n",
    "# Run the complete multi-image pipeline with DINOv2\n",
    "results = multi_image_reid_pipeline(\n",
    "    lost_images=lost_images,\n",
    "    found_images=found_images,\n",
    "    similarity_threshold=0.65,      # DINOv2 typically uses 0.60-0.75 (higher than MegaDescriptor)\n",
    "    aggregation='max',              # Options: 'max', 'mean', 'median', 'top2_mean'\n",
    "    show_all_visualizations=True    # Set to False for faster execution\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecbba0e",
   "metadata": {},
   "source": [
    "## 12. Try Different Aggregation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16805b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different aggregation strategies\n",
    "if results is not None:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ”¬ COMPARING AGGREGATION METHODS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    aggregation_methods = ['max', 'mean', 'median', 'top2_mean']\n",
    "    threshold = 0.55\n",
    "    \n",
    "    comparison_results = []\n",
    "    \n",
    "    for method in aggregation_methods:\n",
    "        match, score, details = make_match_decision(\n",
    "            results['similarity_matrix'],\n",
    "            results['lost_names'],\n",
    "            results['found_names'],\n",
    "            threshold=threshold,\n",
    "            aggregation=method\n",
    "        )\n",
    "        \n",
    "        comparison_results.append({\n",
    "            'method': method,\n",
    "            'score': score,\n",
    "            'match': match\n",
    "        })\n",
    "    \n",
    "    # Display comparison\n",
    "    print(f\"\\nThreshold: {threshold}\\n\")\n",
    "    print(f\"{'Method':<15} {'Score':<10} {'Decision':<20}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for r in comparison_results:\n",
    "        decision_str = \"âœ… MATCH\" if r['match'] else \"âŒ NO MATCH\"\n",
    "        print(f\"{r['method']:<15} {r['score']:<10.4f} {decision_str:<20}\")\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Recommendations:\")\n",
    "    print(\"  - 'max': Best for finding strongest evidence of match\")\n",
    "    print(\"  - 'mean': Best for overall agreement across all images\")\n",
    "    print(\"  - 'median': Best for robustness to outliers\")\n",
    "    print(\"  - 'top2_mean': Balanced approach using best pairs\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2572e3",
   "metadata": {},
   "source": [
    "## 13. Export Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c9df73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def export_results(results, output_file='reid_results.json'):\n",
    "    \"\"\"\n",
    "    Export re-identification results to JSON file\n",
    "    \"\"\"\n",
    "    if results is None:\n",
    "        print(\"No results to export\")\n",
    "        return\n",
    "    \n",
    "    export_data = {\n",
    "        'match': results['match'],\n",
    "        'ensemble_score': float(results['ensemble_score']),\n",
    "        'threshold': results['decision_details']['threshold'],\n",
    "        'aggregation': results['decision_details']['aggregation'],\n",
    "        'best_pair': {\n",
    "            'lost': results['decision_details']['best_pair'][0],\n",
    "            'found': results['decision_details']['best_pair'][1],\n",
    "            'score': float(results['decision_details']['best_score'])\n",
    "        },\n",
    "        'statistics': {\n",
    "            'max': float(results['decision_details']['statistics']['max']),\n",
    "            'mean': float(results['decision_details']['statistics']['mean']),\n",
    "            'median': float(results['decision_details']['statistics']['median']),\n",
    "            'min': float(results['decision_details']['statistics']['min']),\n",
    "            'std': float(results['decision_details']['statistics']['std'])\n",
    "        },\n",
    "        'similarity_matrix': results['similarity_matrix'].tolist(),\n",
    "        'lost_images': results['lost_names'],\n",
    "        'found_images': results['found_names']\n",
    "    }\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(export_data, f, indent=2)\n",
    "    \n",
    "    print(f\"âœ… Results exported to {output_file}\")\n",
    "\n",
    "# Uncomment to export results\n",
    "# export_results(results, 'multi_image_reid_results.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ca88f6",
   "metadata": {},
   "source": [
    "## 14. Understanding Multi-Image Results\n",
    "\n",
    "### ðŸŽ¯ How to Interpret Results:\n",
    "\n",
    "**1. Similarity Matrix:**\n",
    "- Shows pairwise similarity between all lost and found images\n",
    "- Each cell represents one comparison\n",
    "- Green = high similarity, Red = low similarity\n",
    "\n",
    "**2. Ensemble Score:**\n",
    "- Aggregated score from all comparisons\n",
    "- Different aggregation methods give different perspectives:\n",
    "  - **Max**: \"Is there ANY strong match?\" (optimistic)\n",
    "  - **Mean**: \"Do they match ON AVERAGE?\" (balanced)\n",
    "  - **Median**: \"What's the typical similarity?\" (robust)\n",
    "  - **Top2_mean**: \"Do the best pairs agree?\" (conservative)\n",
    "\n",
    "**3. Score Distribution:**\n",
    "- Histogram shows spread of all pairwise scores\n",
    "- Tight distribution â†’ consistent evidence\n",
    "- Wide distribution â†’ mixed evidence\n",
    "\n",
    "### ðŸ’¡ Advantages of Multi-Image Approach:\n",
    "\n",
    "**Increased Robustness:**\n",
    "- One bad photo won't ruin the match\n",
    "- Different angles provide complementary info\n",
    "- Reduces false negatives from poor lighting/pose\n",
    "\n",
    "**Higher Confidence:**\n",
    "- Multiple images agreeing = stronger evidence\n",
    "- Can detect inconsistencies (different pets)\n",
    "- Better handles variation in pet appearance\n",
    "\n",
    "**Flexible Decision Making:**\n",
    "- Can choose aggregation based on use case\n",
    "- Strict matching: use 'mean' or 'median'\n",
    "- Lenient matching: use 'max'\n",
    "\n",
    "### âš™ï¸ Tuning Recommendations:\n",
    "\n",
    "**For Lost & Found Matching:**\n",
    "- Use 'max' aggregation (find best evidence)\n",
    "- Threshold: 0.50-0.55\n",
    "- Require at least 2-3 images per side\n",
    "\n",
    "**For Database Search:**\n",
    "- Use 'mean' or 'top2_mean' aggregation\n",
    "- Threshold: 0.55-0.60 (more conservative)\n",
    "- Store multiple embeddings per pet\n",
    "\n",
    "**If getting false positives:**\n",
    "- Increase threshold to 0.60+\n",
    "- Switch to 'mean' or 'median' aggregation\n",
    "- Require higher consistency across pairs\n",
    "\n",
    "**If missing true matches:**\n",
    "- Decrease threshold to 0.45-0.50\n",
    "- Use 'max' aggregation\n",
    "- Add more diverse images per pet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (visionModel .venv)",
   "language": "python",
   "name": "visionmodel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
