{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "328fda2d",
   "metadata": {},
   "source": [
    "# Complete Pet Re-Identification Pipeline\n",
    "## Ready for API Integration\n",
    "\n",
    "This notebook demonstrates the complete pipeline:\n",
    "1. **Visual Processing**: YOLOv8 detection + DINOv2 embeddings\n",
    "2. **Metadata Matching**: Location, time, physical attributes\n",
    "3. **Final Scoring**: Weighted combination for match confidence\n",
    "\n",
    "### Pipeline Flow\n",
    "```\n",
    "Input (Lost Post) ──► Image Processing ──► Embedding Extraction ──┐\n",
    "                                                                   │\n",
    "Input (Found Post) ─► Image Processing ──► Embedding Extraction ──┤\n",
    "                                                                   │\n",
    "                                                                   ▼\n",
    "                                                          Matching Engine\n",
    "                                                                   │\n",
    "                                                                   ├─► Visual Similarity (50%)\n",
    "                                                                   ├─► Time Score (20%)\n",
    "                                                                   ├─► Location Score (20%)\n",
    "                                                                   ├─► Gender Score (10%)\n",
    "                                                                   │\n",
    "                                                                   ▼\n",
    "                                                          Match Result + Confidence\n",
    "```\n",
    "\n",
    "### Current Weighting System\n",
    "- **Visual Match**: 50% - Image similarity using DINOv2\n",
    "- **Time**: 20% - Recency with exponential decay\n",
    "- **Location**: 20% - Distance with exponential decay  \n",
    "- **Gender**: 10% - Gender matching component\n",
    "- **Pet Type**: Must match (cat/dog/bird/rabbit) - multiplier of 1.0 or 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c040ba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "!pip install torch torchvision transformers ultralytics opencv-python pillow matplotlib numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0930664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Import vision processing\n",
    "from ultralytics import YOLO\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "# Import our matching system\n",
    "from pet_matching_engine import (\n",
    "    PetPost, MatchResult, PetMatcher, PetMatchingConfig,\n",
    "    format_match_result\n",
    ")\n",
    "\n",
    "print(\"✓ All modules imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb1b493",
   "metadata": {},
   "source": [
    "## Step 1: Load Models\n",
    "\n",
    "Load YOLOv8-Seg and DINOv2 models with caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c63222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model cache directory\n",
    "cache_dir = Path('models_cache')\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Loading models...\\n\")\n",
    "\n",
    "# 1. YOLOv8 Segmentation Model\n",
    "print(\"[1/2] Loading YOLOv8-Seg...\")\n",
    "yolo_path = cache_dir / 'yolov8x-seg.pt'\n",
    "if not yolo_path.exists():\n",
    "    print(\"  Downloading YOLOv8-Seg (~131MB)...\")\n",
    "    detector = YOLO('yolov8x-seg.pt')\n",
    "    detector.model.to(device)\n",
    "else:\n",
    "    print(\"  Loading from cache...\")\n",
    "    detector = YOLO(str(yolo_path))\n",
    "    detector.model.to(device)\n",
    "print(\"  ✓ YOLOv8-Seg loaded\\n\")\n",
    "\n",
    "# 2. DINOv2 Model\n",
    "print(\"[2/2] Loading DINOv2-Large...\")\n",
    "model_name = 'facebook/dinov2-large'\n",
    "processor = AutoImageProcessor.from_pretrained(model_name, cache_dir=str(cache_dir), use_fast=True)\n",
    "reid_model = AutoModel.from_pretrained(model_name, cache_dir=str(cache_dir))\n",
    "reid_model = reid_model.to(device)\n",
    "reid_model.eval()\n",
    "print(\"  ✓ DINOv2-Large loaded\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"All models loaded successfully!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5205f7ae",
   "metadata": {},
   "source": [
    "## Step 2: Define Processing Functions\n",
    "\n",
    "Functions for detection, preprocessing, and embedding extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4575a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_pets_with_segmentation(image_path: str, conf_threshold: float = 0.3) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Detect pets using YOLOv8 segmentation\n",
    "    Returns list of detections with bounding boxes and segmentation masks\n",
    "    \"\"\"\n",
    "    results = detector(image_path, conf=conf_threshold, verbose=False)\n",
    "    detections = []\n",
    "    \n",
    "    for result in results:\n",
    "        if result.masks is None:\n",
    "            continue\n",
    "            \n",
    "        boxes = result.boxes\n",
    "        masks = result.masks.data.cpu().numpy()\n",
    "        \n",
    "        for idx, (box, mask) in enumerate(zip(boxes, masks)):\n",
    "            class_id = int(box.cls[0])\n",
    "            class_name = result.names[class_id]\n",
    "            \n",
    "            # Filter for cats (15) and dogs (16)\n",
    "            if class_id not in [15, 16]:\n",
    "                continue\n",
    "            \n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            confidence = float(box.conf[0])\n",
    "            \n",
    "            detections.append({\n",
    "                'bbox': (x1, y1, x2, y2),\n",
    "                'class': class_name,\n",
    "                'confidence': confidence,\n",
    "                'mask': mask\n",
    "            })\n",
    "    \n",
    "    return detections\n",
    "\n",
    "\n",
    "def preprocess_crop_advanced(image: np.ndarray, detection: Dict, \n",
    "                            padding_percent: float = 0.15) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Advanced preprocessing with background removal and enhancement\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = detection['bbox']\n",
    "    mask = detection['mask']\n",
    "    \n",
    "    # Resize mask to image dimensions\n",
    "    h, w = image.shape[:2]\n",
    "    mask_resized = cv2.resize(mask, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "    mask_binary = (mask_resized > 0.5).astype(np.uint8) * 255\n",
    "    \n",
    "    # Create masked image (remove background)\n",
    "    masked_image = cv2.bitwise_and(image, image, mask=mask_binary)\n",
    "    \n",
    "    # Add padding to bounding box\n",
    "    pad_w = int((x2 - x1) * padding_percent)\n",
    "    pad_h = int((y2 - y1) * padding_percent)\n",
    "    \n",
    "    x1_pad = max(0, x1 - pad_w)\n",
    "    y1_pad = max(0, y1 - pad_h)\n",
    "    x2_pad = min(w, x2 + pad_w)\n",
    "    y2_pad = min(h, y2 + pad_h)\n",
    "    \n",
    "    # Crop with padding\n",
    "    cropped = masked_image[y1_pad:y2_pad, x1_pad:x2_pad]\n",
    "    \n",
    "    # CLAHE enhancement on LAB color space\n",
    "    lab = cv2.cvtColor(cropped, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    l_clahe = clahe.apply(l)\n",
    "    enhanced = cv2.merge([l_clahe, a, b])\n",
    "    enhanced = cv2.cvtColor(enhanced, cv2.COLOR_LAB2BGR)\n",
    "    \n",
    "    return enhanced\n",
    "\n",
    "\n",
    "def extract_embedding_with_tta(image_bgr: np.ndarray, \n",
    "                              processor, model, device) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract DINOv2 embedding with test-time augmentation\n",
    "    Averages embeddings from original and horizontally flipped image\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    \n",
    "    # Convert BGR to RGB\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "    pil_image = Image.fromarray(image_rgb)\n",
    "    \n",
    "    # Original image\n",
    "    inputs = processor(images=pil_image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()[0]\n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    # Horizontally flipped image\n",
    "    pil_image_flip = pil_image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    inputs_flip = processor(images=pil_image_flip, return_tensors=\"pt\")\n",
    "    inputs_flip = {k: v.to(device) for k, v in inputs_flip.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs_flip = model(**inputs_flip)\n",
    "        embedding_flip = outputs_flip.last_hidden_state[:, 0, :].cpu().numpy()[0]\n",
    "        embeddings.append(embedding_flip)\n",
    "    \n",
    "    # Average embeddings\n",
    "    final_embedding = np.mean(embeddings, axis=0)\n",
    "    \n",
    "    # Normalize\n",
    "    final_embedding = final_embedding / np.linalg.norm(final_embedding)\n",
    "    \n",
    "    return final_embedding\n",
    "\n",
    "\n",
    "def process_pet_images(image_paths: List[str]) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Complete processing pipeline for multiple images\n",
    "    Returns list of embeddings\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for img_path in image_paths:\n",
    "        print(f\"  Processing: {Path(img_path).name}\")\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(str(img_path))\n",
    "        if image is None:\n",
    "            print(f\"    ⚠ Warning: Could not load image\")\n",
    "            continue\n",
    "        \n",
    "        # Detect pets\n",
    "        detections = detect_pets_with_segmentation(str(img_path))\n",
    "        \n",
    "        if not detections:\n",
    "            print(f\"    ⚠ Warning: No pets detected\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"    Found {len(detections)} pet(s)\")\n",
    "        \n",
    "        # Process each detection\n",
    "        for idx, det in enumerate(detections):\n",
    "            # Preprocess\n",
    "            processed = preprocess_crop_advanced(image, det)\n",
    "            \n",
    "            # Extract embedding\n",
    "            embedding = extract_embedding_with_tta(processed, processor, reid_model, device)\n",
    "            all_embeddings.append(embedding)\n",
    "            \n",
    "            print(f\"    ✓ Extracted embedding (detection {idx+1})\")\n",
    "    \n",
    "    return all_embeddings\n",
    "\n",
    "\n",
    "print(\"✓ Processing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e205a56f",
   "metadata": {},
   "source": [
    "## Step 3: Complete Matching Function (API-Ready)\n",
    "\n",
    "This is the main function that will be wrapped in your API endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abe52cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_pet_posts(lost_post_data: Dict, found_post_data: Dict, \n",
    "                   detailed: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Complete matching pipeline - ready for API integration\n",
    "    \n",
    "    Args:\n",
    "        lost_post_data: Dictionary with lost pet information\n",
    "            {\n",
    "                'id': str,\n",
    "                'pet_type': str,  # 'cat' or 'dog'\n",
    "                'description': str (optional),\n",
    "                'latitude': float,\n",
    "                'longitude': float,\n",
    "                'timestamp': datetime,\n",
    "                'image_paths': List[str],  # paths to images\n",
    "                'contact_info': str (optional),\n",
    "                'neutered': bool (optional),\n",
    "                'gender': str (optional)  # 'male' or 'female'\n",
    "            }\n",
    "        found_post_data: Dictionary with found pet information (same structure)\n",
    "        detailed: Include detailed breakdown in response\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with match result\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STARTING MATCH PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 1: Process lost pet images\n",
    "    print(\"\\n[1/3] Processing LOST pet images...\")\n",
    "    lost_embeddings = process_pet_images(lost_post_data['image_paths'])\n",
    "    print(f\"  ✓ Extracted {len(lost_embeddings)} embeddings from lost pet\\n\")\n",
    "    \n",
    "    # Step 2: Process found pet images\n",
    "    print(\"[2/3] Processing FOUND pet images...\")\n",
    "    found_embeddings = process_pet_images(found_post_data['image_paths'])\n",
    "    print(f\"  ✓ Extracted {len(found_embeddings)} embeddings from found pet\\n\")\n",
    "    \n",
    "    # Create PetPost objects\n",
    "    lost_post = PetPost(\n",
    "        id=lost_post_data['id'],\n",
    "        pet_type=lost_post_data['pet_type'],\n",
    "        description=lost_post_data.get('description'),\n",
    "        latitude=lost_post_data['latitude'],\n",
    "        longitude=lost_post_data['longitude'],\n",
    "        timestamp=lost_post_data['timestamp'],\n",
    "        # is_lost=True,  # Not part of PetPost class\n",
    "        neutered=lost_post_data.get('neutered'),\n",
    "        gender=lost_post_data.get('gender'),\n",
    "        embeddings=lost_embeddings\n",
    "    )\n",
    "    \n",
    "    found_post = PetPost(\n",
    "        id=found_post_data['id'],\n",
    "        pet_type=found_post_data['pet_type'],\n",
    "        description=found_post_data.get('description'),\n",
    "        latitude=found_post_data['latitude'],\n",
    "        longitude=found_post_data['longitude'],\n",
    "        timestamp=found_post_data['timestamp'],\n",
    "        # is_lost=False,  # Not part of PetPost class\n",
    "        neutered=found_post_data.get('neutered'),\n",
    "        gender=found_post_data.get('gender'),\n",
    "        embeddings=found_embeddings\n",
    "    )\n",
    "    \n",
    "    # Step 3: Perform matching\n",
    "    print(\"[3/3] Computing match score...\")\n",
    "    matcher = PetMatcher()\n",
    "    result = matcher.match(lost_post, found_post)\n",
    "    \n",
    "    # Format result\n",
    "    output = format_match_result(result, detailed=detailed)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MATCH RESULT\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Confidence: {result.confidence:.2f}%\")\n",
    "    print(f\"Category: {result.match_category.upper()}\")\n",
    "    print(f\"Is Match: {result.is_match}\")\n",
    "    \n",
    "    if detailed:\n",
    "        print(\"\\nBreakdown:\")\n",
    "        print(f\"  Visual Similarity: {result.visual_similarity:.4f}\")\n",
    "        print(f\"  Location Score: {result.location_score:.2f}/100 ({result.distance_km:.2f} km)\")\n",
    "        print(f\"  Time Score: {result.time_score:.2f}/100 ({result.time_diff_hours:.1f} hrs)\")\n",
    "        print(f\"  Metadata Score: {result.metadata_score:.2f}/100\")\n",
    "    \n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "print(\"✓ API-ready matching function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182ba81e",
   "metadata": {},
   "source": [
    "## Step 4: Example Usage\n",
    "\n",
    "Test the complete pipeline with sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac5be92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Lost Pet Post\n",
    "lost_pet = {\n",
    "    'id': 'LOST_001',\n",
    "    'pet_type': 'cat',\n",
    "    'description': 'Orange tabby with white paws, small scar on left ear',\n",
    "    'latitude': 23.8103,  # Dhaka coordinates\n",
    "    'longitude': 90.4125,\n",
    "    'timestamp': datetime(2025, 10, 10, 14, 30),\n",
    "    'image_paths': [\n",
    "        'images/a.jpg',\n",
    "        'images/b.jpg',\n",
    "        'images/c.jpg'\n",
    "    ],\n",
    "    'contact_info': '+880-123-456789',\n",
    "    'neutered': True,\n",
    "    'gender': 'male'\n",
    "}\n",
    "\n",
    "# Example: Found Pet Post\n",
    "found_pet = {\n",
    "    'id': 'FOUND_001',\n",
    "    'pet_type': 'cat',\n",
    "    'description': 'Orange cat with white markings, friendly',\n",
    "    'latitude': 23.8150,  # ~520m away\n",
    "    'longitude': 90.4180,\n",
    "    'timestamp': datetime(2025, 10, 12, 9, 15),\n",
    "    'image_paths': [\n",
    "        'images/p.jpg',\n",
    "        'images/q.jpg'\n",
    "    ],\n",
    "    'contact_info': '+880-987-654321',\n",
    "    'neutered': True,\n",
    "    'gender': 'male'\n",
    "}\n",
    "\n",
    "print(\"Example data structures created\")\n",
    "print(\"\\nTo run matching, execute:\")\n",
    "print(\">>> result = match_pet_posts(lost_pet, found_pet)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6387b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the matching pipeline\n",
    "# Adjust image paths as needed\n",
    "\n",
    "result = match_pet_posts(lost_pet, found_pet, detailed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba865711",
   "metadata": {},
   "source": [
    "## Step 5: Batch Matching (Find Best Matches)\n",
    "\n",
    "Match one lost pet against multiple found pets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aedce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches_for_lost_pet(lost_post_data: Dict, \n",
    "                             found_posts_data: List[Dict],\n",
    "                             top_k: int = 5,\n",
    "                             min_confidence: float = 45.0) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Find best matches for a lost pet from multiple found pets\n",
    "    \n",
    "    Args:\n",
    "        lost_post_data: Lost pet information\n",
    "        found_posts_data: List of found pet information\n",
    "        top_k: Return top K matches\n",
    "        min_confidence: Minimum confidence threshold\n",
    "    \n",
    "    Returns:\n",
    "        List of match results sorted by confidence\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"BATCH MATCHING: 1 LOST PET vs {len(found_posts_data)} FOUND PETS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Process lost pet once\n",
    "    print(\"\\nProcessing LOST pet images...\")\n",
    "    lost_embeddings = process_pet_images(lost_post_data['image_paths'])\n",
    "    \n",
    "    lost_post = PetPost(\n",
    "        id=lost_post_data['id'],\n",
    "        pet_type=lost_post_data['pet_type'],\n",
    "        description=lost_post_data.get('description'),\n",
    "        latitude=lost_post_data['latitude'],\n",
    "        longitude=lost_post_data['longitude'],\n",
    "        timestamp=lost_post_data['timestamp'],\n",
    "        # is_lost=True,  # Not part of PetPost class\n",
    "        neutered=lost_post_data.get('neutered'),\n",
    "        gender=lost_post_data.get('gender'),\n",
    "        embeddings=lost_embeddings\n",
    "    )\n",
    "    \n",
    "    # Process each found pet and compute match\n",
    "    found_posts = []\n",
    "    \n",
    "    for i, found_data in enumerate(found_posts_data, 1):\n",
    "        print(f\"\\nProcessing FOUND pet {i}/{len(found_posts_data)}...\")\n",
    "        \n",
    "        found_embeddings = process_pet_images(found_data['image_paths'])\n",
    "        \n",
    "        found_post = PetPost(\n",
    "            id=found_data['id'],\n",
    "            pet_type=found_data['pet_type'],\n",
    "            description=found_data.get('description'),\n",
    "            latitude=found_data['latitude'],\n",
    "            longitude=found_data['longitude'],\n",
    "            timestamp=found_data['timestamp'],\n",
    "            # is_lost=False,  # Not part of PetPost class\n",
    "            neutered=found_data.get('neutered'),\n",
    "            gender=found_data.get('gender'),\n",
    "            embeddings=found_embeddings\n",
    "        )\n",
    "        \n",
    "        found_posts.append(found_post)\n",
    "    \n",
    "    # Find best matches\n",
    "    print(\"\\nComputing matches...\")\n",
    "    matcher = PetMatcher()\n",
    "    matches = matcher.find_best_matches(\n",
    "        lost_post, found_posts, \n",
    "        top_k=top_k, \n",
    "        min_confidence=min_confidence\n",
    "    )\n",
    "    \n",
    "    # Format results\n",
    "    results = [format_match_result(m, detailed=True) for m in matches]\n",
    "    \n",
    "    # Display summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MATCH RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total candidates: {len(found_posts_data)}\")\n",
    "    print(f\"Matches found: {len(results)}\\n\")\n",
    "    \n",
    "    for i, r in enumerate(results, 1):\n",
    "        print(f\"{i}. {r['matched_id']} - Confidence: {r['confidence']}% ({r['match_category']})\")\n",
    "    \n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"✓ Batch matching function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c816e632",
   "metadata": {},
   "source": [
    "## Step 6: Visualization\n",
    "\n",
    "Visualize match results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e6b6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_match_result(result: Dict, \n",
    "                          lost_image_path: str, \n",
    "                          found_image_path: str):\n",
    "    \"\"\"\n",
    "    Visualize match comparison between two images\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "    \n",
    "    # Lost pet image\n",
    "    lost_img = cv2.imread(lost_image_path)\n",
    "    lost_img = cv2.cvtColor(lost_img, cv2.COLOR_BGR2RGB)\n",
    "    axes[0].imshow(lost_img)\n",
    "    axes[0].set_title(f\"LOST PET\\nID: {result['query_id']}\", fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Found pet image\n",
    "    found_img = cv2.imread(found_image_path)\n",
    "    found_img = cv2.cvtColor(found_img, cv2.COLOR_BGR2RGB)\n",
    "    axes[1].imshow(found_img)\n",
    "    axes[1].set_title(f\"FOUND PET\\nID: {result['matched_id']}\", fontsize=14, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Match result\n",
    "    confidence = result['confidence']\n",
    "    category = result['match_category'].upper()\n",
    "    is_match = result['is_match']\n",
    "    \n",
    "    if is_match and confidence >= 75:\n",
    "        color = 'green'\n",
    "        status = '✓ HIGH CONFIDENCE MATCH'\n",
    "    elif is_match:\n",
    "        color = 'orange'\n",
    "        status = '⚠ MEDIUM CONFIDENCE MATCH'\n",
    "    else:\n",
    "        color = 'red'\n",
    "        status = '✗ NO MATCH'\n",
    "    \n",
    "    fig.suptitle(\n",
    "        f\"{status}\\nConfidence: {confidence:.1f}% | Category: {category}\",\n",
    "        fontsize=16, fontweight='bold', color=color, y=0.98\n",
    "    )\n",
    "    \n",
    "    # Details\n",
    "    if 'details' in result:\n",
    "        details = result['details']\n",
    "        detail_text = (\n",
    "            f\"Visual Similarity: {details['visual_similarity']:.3f}\\n\"\n",
    "            f\"Distance: {details['distance_km']:.2f} km\\n\"\n",
    "            f\"Time Difference: {details['time_diff_hours']:.1f} hours\"\n",
    "        )\n",
    "        fig.text(0.5, 0.02, detail_text, ha='center', fontsize=11, \n",
    "                family='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"✓ Visualization function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bc56bd",
   "metadata": {},
   "source": [
    "## Configuration & Tuning Guide\n",
    "\n",
    "### Current Weight Configuration\n",
    "\n",
    "The matching system uses the following weights:\n",
    "- **Visual (Image Match): 50%** - Most important factor\n",
    "- **Time: 20%** - Temporal proximity\n",
    "- **Location: 20%** - Spatial proximity\n",
    "- **Gender: 10%** - Gender matching component\n",
    "- **Pet Type Multiplier: × 1.0 or × 0.0** - Must match (cat/dog/bird/rabbit)\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "weighted_score = (0.50 × visual_score) + (0.20 × time_score) + (0.20 × location_score) + (0.10 × gender_score)\n",
    "final_score = weighted_score × pet_type_multiplier\n",
    "```\n",
    "\n",
    "### Custom Configuration (Advanced)\n",
    "\n",
    "Modify `PetMatchingConfig` class for different scenarios:\n",
    "\n",
    "```python\n",
    "# Custom matcher with adjusted thresholds\n",
    "config = PetMatchingConfig()\n",
    "\n",
    "# Distance thresholds (exponential decay)\n",
    "config.DISTANCE_PERFECT_MATCH_KM = 10.0  # Perfect match zone\n",
    "config.DISTANCE_HALF_LIFE_KM = 15.0      # Half-life for decay\n",
    "\n",
    "# Time thresholds (exponential decay)\n",
    "config.TIME_PERFECT_MATCH_DAYS = 3.0     # Perfect match zone\n",
    "config.TIME_HALF_LIFE_DAYS = 14.0        # Half-life for decay\n",
    "config.TIME_MAX_DAYS = 90                # Hard cutoff\n",
    "\n",
    "# Confidence thresholds\n",
    "config.CONFIDENCE_HIGH = 75.0            # High confidence match\n",
    "config.CONFIDENCE_MEDIUM = 60.0          # Medium confidence\n",
    "config.CONFIDENCE_LOW = 45.0             # Low confidence\n",
    "\n",
    "# Weight configuration\n",
    "config.WEIGHT_VISUAL = 0.50              # Visual similarity weight (50%)\n",
    "config.WEIGHT_TIME = 0.20                # Time proximity weight (20%)\n",
    "config.WEIGHT_LOCATION = 0.20            # Location proximity weight (20%)\n",
    "config.WEIGHT_GENDER = 0.10              # Gender match weight (10%)\n",
    "\n",
    "matcher = PetMatcher(config)\n",
    "```\n",
    "\n",
    "### API Integration Notes\n",
    "\n",
    "1. **Image Storage**: Save uploaded images with unique IDs\n",
    "2. **Caching**: Cache embeddings in database to avoid reprocessing\n",
    "3. **Async Processing**: Use background tasks for batch matching\n",
    "4. **Rate Limiting**: Limit API calls to prevent abuse\n",
    "5. **Error Handling**: Validate all inputs, handle missing images gracefully\n",
    "\n",
    "### Expected Performance\n",
    "\n",
    "- **Same Pet (Good Images)**: 75-90% confidence\n",
    "- **Same Pet (Different Poses)**: 60-75% confidence\n",
    "- **Similar Looking Pets**: 45-60% confidence\n",
    "- **Different Pets**: <45% confidence\n",
    "\n",
    "### Match Categories\n",
    "\n",
    "- **High (≥75%)**: Very likely the same pet, notify owner immediately\n",
    "- **Medium (60-74%)**: Probable match, show for manual verification\n",
    "- **Low (45-59%)**: Possible match, include in extended results\n",
    "- **No Match (<45%)**: Unlikely to be the same pet\n",
    "\n",
    "### Next Steps for API\n",
    "\n",
    "1. Wrap `match_pet_posts()` in FastAPI/Flask endpoint\n",
    "2. Add database integration for caching embeddings\n",
    "3. Implement batch processing queue\n",
    "4. Add authentication & rate limiting\n",
    "5. Deploy with GPU support for fast inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (visionModel .venv)",
   "language": "python",
   "name": "visionmodel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
